package main

import (
	"context"
	"fmt"
	"math/rand"
	"sync"
	"time"
)

// URL ä»£è¡¨ä¸€ä¸ªå¾…æŠ“å–çš„ç½‘é¡µ
type URL struct {
	Address string
	Depth   int
}

// PageContent ä»£è¡¨æŠ“å–çš„é¡µé¢å†…å®¹
type PageContent struct {
	URL     string
	Content string
	Links   []string
	Error   error
}

// ç¤ºä¾‹1: ç®€å•çš„å¹¶å‘ç½‘é¡µæŠ“å–å™¨
func example1_simple_scraper() {
	fmt.Println("\n=== ç¤ºä¾‹1: ç®€å•å¹¶å‘æŠ“å–å™¨ ===")
	
	// æ¨¡æ‹ŸæŠ“å–ç½‘é¡µ
	fetchPage := func(url string) (PageContent, error) {
		// æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
		time.Sleep(time.Duration(100+rand.Intn(300)) * time.Millisecond)
		
		// æ¨¡æ‹Ÿ10%çš„å¤±è´¥ç‡
		if rand.Float32() < 0.1 {
			return PageContent{}, fmt.Errorf("æŠ“å–å¤±è´¥: %s", url)
		}
		
		return PageContent{
			URL:     url,
			Content: fmt.Sprintf("å†…å®¹æ¥è‡ª %s", url),
			Links:   []string{},
		}, nil
	}
	
	urls := []string{
		"https://example.com/page1",
		"https://example.com/page2",
		"https://example.com/page3",
		"https://example.com/page4",
		"https://example.com/page5",
	}
	
	results := make(chan PageContent, len(urls))
	
	// å¹¶å‘æŠ“å–
	var wg sync.WaitGroup
	for _, url := range urls {
		wg.Add(1)
		go func(u string) {
			defer wg.Done()
			fmt.Printf("å¼€å§‹æŠ“å–: %s\n", u)
			page, err := fetchPage(u)
			if err != nil {
				results <- PageContent{URL: u, Error: err}
			} else {
				results <- page
			}
		}(url)
	}
	
	// ç­‰å¾…æ‰€æœ‰æŠ“å–å®Œæˆ
	go func() {
		wg.Wait()
		close(results)
	}()
	
	// æ”¶é›†ç»“æœ
	successCount := 0
	failCount := 0
	for page := range results {
		if page.Error != nil {
			fmt.Printf("âŒ %s: %v\n", page.URL, page.Error)
			failCount++
		} else {
			fmt.Printf("âœ… %s: %s\n", page.URL, page.Content)
			successCount++
		}
	}
	
	fmt.Printf("\nç»Ÿè®¡: æˆåŠŸ %d, å¤±è´¥ %d\n", successCount, failCount)
}

// ç¤ºä¾‹2: å¸¦é™æµçš„æŠ“å–å™¨
func example2_rate_limited_scraper() {
	fmt.Println("\n=== ç¤ºä¾‹2: å¸¦é™æµçš„æŠ“å–å™¨ ===")
	
	// é™æµå™¨ï¼šæ¯ç§’æœ€å¤š2ä¸ªè¯·æ±‚
	type RateLimiter struct {
		tokens chan struct{}
	}
	
	NewRateLimiter := func(requestsPerSecond int) *RateLimiter {
		rl := &RateLimiter{
			tokens: make(chan struct{}, requestsPerSecond),
		}
		
		// å®šæœŸè¡¥å……ä»¤ç‰Œ
		go func() {
			ticker := time.NewTicker(time.Second / time.Duration(requestsPerSecond))
			defer ticker.Stop()
			for range ticker.C {
				select {
				case rl.tokens <- struct{}{}:
				default:
				}
			}
		}()
		
		return rl
	}
	
	fetchWithLimit := func(limiter *RateLimiter, url string) PageContent {
		<-limiter.tokens // è·å–ä»¤ç‰Œ
		
		fmt.Printf("[%s] æŠ“å–: %s\n", time.Now().Format("15:04:05"), url)
		time.Sleep(100 * time.Millisecond)
		
		return PageContent{
			URL:     url,
			Content: fmt.Sprintf("å†…å®¹: %s", url),
		}
	}
	
	limiter := NewRateLimiter(2)
	urls := []string{"page1", "page2", "page3", "page4", "page5", "page6"}
	
	var wg sync.WaitGroup
	for _, url := range urls {
		wg.Add(1)
		go func(u string) {
			defer wg.Done()
			fetchWithLimit(limiter, u)
		}(url)
	}
	
	wg.Wait()
}

// ç¤ºä¾‹3: æ·±åº¦ä¼˜å…ˆçˆ¬è™«ï¼ˆé€’å½’æŠ“å–é“¾æ¥ï¼‰
func example3_depth_first_crawler() {
	fmt.Println("\n=== ç¤ºä¾‹3: æ·±åº¦ä¼˜å…ˆçˆ¬è™« ===")
	
	// æ¨¡æ‹ŸæŠ“å–é¡µé¢ï¼ˆè¿”å›é“¾æ¥ï¼‰
	fetchPageWithLinks := func(url string) PageContent {
		time.Sleep(100 * time.Millisecond)
		
		// æ¨¡æ‹Ÿç”Ÿæˆå­é“¾æ¥
		var links []string
		numLinks := rand.Intn(3)
		for i := 0; i < numLinks; i++ {
			links = append(links, fmt.Sprintf("%s/link%d", url, i+1))
		}
		
		return PageContent{
			URL:     url,
			Content: fmt.Sprintf("é¡µé¢: %s", url),
			Links:   links,
		}
	}
	
	// çˆ¬è™«
	crawl := func(url string, maxDepth int) {
		visited := make(map[string]bool)
		var mu sync.Mutex
		
		var crawlRecursive func(string, int)
		crawlRecursive = func(u string, depth int) {
			if depth > maxDepth {
				return
			}
			
			// æ£€æŸ¥æ˜¯å¦å·²è®¿é—®
			mu.Lock()
			if visited[u] {
				mu.Unlock()
				return
			}
			visited[u] = true
			mu.Unlock()
			
			// æŠ“å–é¡µé¢
			fmt.Printf("æ·±åº¦ %d: %s\n", depth, u)
			page := fetchPageWithLinks(u)
			
			// é€’å½’æŠ“å–é“¾æ¥
			var wg sync.WaitGroup
			for _, link := range page.Links {
				wg.Add(1)
				go func(l string) {
					defer wg.Done()
					crawlRecursive(l, depth+1)
				}(link)
			}
			wg.Wait()
		}
		
		crawlRecursive(url, 0)
	}
	
	crawl("https://example.com", 2)
}

// ç¤ºä¾‹4: å¹¿åº¦ä¼˜å…ˆçˆ¬è™«ï¼ˆä½¿ç”¨é˜Ÿåˆ—ï¼‰
func example4_breadth_first_crawler() {
	fmt.Println("\n=== ç¤ºä¾‹4: å¹¿åº¦ä¼˜å…ˆçˆ¬è™« ===")
	
	fetchPageWithLinks := func(url string) PageContent {
		time.Sleep(100 * time.Millisecond)
		
		var links []string
		numLinks := rand.Intn(2) + 1
		for i := 0; i < numLinks; i++ {
			links = append(links, fmt.Sprintf("%s/sub%d", url, i+1))
		}
		
		return PageContent{
			URL:     url,
			Content: fmt.Sprintf("å†…å®¹: %s", url),
			Links:   links,
		}
	}
	
	crawlBFS := func(startURL string, maxDepth int) {
		visited := make(map[string]bool)
		queue := make(chan URL, 100)
		results := make(chan PageContent, 100)
		
		// å¯åŠ¨ worker pool
		const numWorkers = 3
		var wg sync.WaitGroup
		
		for w := 1; w <= numWorkers; w++ {
			wg.Add(1)
			go func(id int) {
				defer wg.Done()
				for url := range queue {
					if url.Depth > maxDepth {
						continue
					}
					
					fmt.Printf("Worker %d æŠ“å–æ·±åº¦ %d: %s\n", id, url.Depth, url.Address)
					page := fetchPageWithLinks(url.Address)
					results <- page
					
					// å°†æ–°é“¾æ¥åŠ å…¥é˜Ÿåˆ—
					for _, link := range page.Links {
						queue <- URL{Address: link, Depth: url.Depth + 1}
					}
				}
			}(w)
		}
		
		// å‘é€èµ·å§‹ URL
		visited[startURL] = true
		queue <- URL{Address: startURL, Depth: 0}
		
		// æ”¶é›†ç»“æœ
		go func() {
			count := 0
			maxCount := 10 // é™åˆ¶æŠ“å–æ•°é‡
			for page := range results {
				fmt.Printf("æ”¶åˆ°: %s (åŒ…å« %d ä¸ªé“¾æ¥)\n", page.URL, len(page.Links))
				count++
				
				if count >= maxCount {
					close(queue)
					return
				}
			}
		}()
		
		wg.Wait()
		close(results)
	}
	
	crawlBFS("https://example.com", 3)
}

// ç¤ºä¾‹5: å¸¦è¶…æ—¶å’Œé‡è¯•çš„æŠ“å–å™¨
func example5_scraper_with_retry() {
	fmt.Println("\n=== ç¤ºä¾‹5: å¸¦è¶…æ—¶å’Œé‡è¯•çš„æŠ“å–å™¨ ===")
	
	fetchWithRetry := func(ctx context.Context, url string, maxRetries int) (PageContent, error) {
		for attempt := 1; attempt <= maxRetries; attempt++ {
			fmt.Printf("å°è¯• %d/%d: %s\n", attempt, maxRetries, url)
			
			// æ¯æ¬¡å°è¯•æœ‰500msè¶…æ—¶
			attemptCtx, cancel := context.WithTimeout(ctx, 500*time.Millisecond)
			
			resultCh := make(chan PageContent, 1)
			errCh := make(chan error, 1)
			
			go func() {
				// æ¨¡æ‹ŸæŠ“å–
				delay := time.Duration(rand.Intn(800)) * time.Millisecond
				time.Sleep(delay)
				
				// æ¨¡æ‹Ÿå¤±è´¥
				if rand.Float32() < 0.4 {
					errCh <- fmt.Errorf("æŠ“å–å¤±è´¥")
					return
				}
				
				resultCh <- PageContent{
					URL:     url,
					Content: fmt.Sprintf("å†…å®¹: %s", url),
				}
			}()
			
			select {
			case page := <-resultCh:
				cancel()
				fmt.Printf("âœ… æˆåŠŸ: %s\n", url)
				return page, nil
			case err := <-errCh:
				cancel()
				fmt.Printf("âŒ å¤±è´¥: %v\n", err)
				if attempt < maxRetries {
					time.Sleep(200 * time.Millisecond)
				}
			case <-attemptCtx.Done():
				cancel()
				fmt.Printf("â±ï¸  è¶…æ—¶\n")
				if attempt < maxRetries {
					time.Sleep(200 * time.Millisecond)
				}
			case <-ctx.Done():
				cancel()
				return PageContent{}, ctx.Err()
			}
		}
		
		return PageContent{}, fmt.Errorf("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
	}
	
	ctx := context.Background()
	urls := []string{"page1", "page2", "page3"}
	
	for _, url := range urls {
		_, err := fetchWithRetry(ctx, url, 3)
		if err != nil {
			fmt.Printf("æœ€ç»ˆå¤±è´¥: %s - %v\n\n", url, err)
		}
	}
}

// ç¤ºä¾‹6: å®Œæ•´çš„ç½‘é¡µçˆ¬è™«ç³»ç»Ÿ
func example6_complete_crawler() {
	fmt.Println("\n=== ç¤ºä¾‹6: å®Œæ•´çˆ¬è™«ç³»ç»Ÿ ===")
	
	type Crawler struct {
		maxWorkers  int
		maxDepth    int
		visited     map[string]bool
		visitedLock sync.RWMutex
		urlQueue    chan URL
		results     chan PageContent
		wg          sync.WaitGroup
	}
	
	NewCrawler := func(maxWorkers, maxDepth int) *Crawler {
		return &Crawler{
			maxWorkers: maxWorkers,
			maxDepth:   maxDepth,
			visited:    make(map[string]bool),
			urlQueue:   make(chan URL, 100),
			results:    make(chan PageContent, 100),
		}
	}
	
	isVisited := func(c *Crawler, url string) bool {
		c.visitedLock.RLock()
		defer c.visitedLock.RUnlock()
		return c.visited[url]
	}
	
	markVisited := func(c *Crawler, url string) bool {
		c.visitedLock.Lock()
		defer c.visitedLock.Unlock()
		
		if c.visited[url] {
			return false
		}
		c.visited[url] = true
		return true
	}
	
	worker := func(c *Crawler, id int, ctx context.Context) {
		defer c.wg.Done()
		
		for {
			select {
			case url, ok := <-c.urlQueue:
				if !ok {
					return
				}
				
				if url.Depth > c.maxDepth {
					continue
				}
				
				// æ£€æŸ¥æ˜¯å¦å·²è®¿é—®
				if !markVisited(c, url.Address) {
					continue
				}
				
				// æŠ“å–é¡µé¢
				fmt.Printf("Worker %d: æŠ“å– [æ·±åº¦ %d] %s\n", id, url.Depth, url.Address)
				time.Sleep(time.Duration(100+rand.Intn(200)) * time.Millisecond)
				
				// æ¨¡æ‹Ÿç”Ÿæˆé“¾æ¥
				var links []string
				if url.Depth < c.maxDepth {
					numLinks := rand.Intn(3)
					for i := 0; i < numLinks; i++ {
						link := fmt.Sprintf("%s/page%d", url.Address, i+1)
						if !isVisited(c, link) {
							links = append(links, link)
						}
					}
				}
				
				page := PageContent{
					URL:     url.Address,
					Content: fmt.Sprintf("å†…å®¹: %s", url.Address),
					Links:   links,
				}
				
				c.results <- page
				
				// æ·»åŠ æ–°é“¾æ¥åˆ°é˜Ÿåˆ—
				for _, link := range links {
					c.urlQueue <- URL{Address: link, Depth: url.Depth + 1}
				}
				
			case <-ctx.Done():
				return
			}
		}
	}
	
	start := func(c *Crawler, ctx context.Context, startURL string) <-chan PageContent {
		// å¯åŠ¨ workers
		for i := 1; i <= c.maxWorkers; i++ {
			c.wg.Add(1)
			go worker(c, i, ctx)
		}
		
		// å‘é€èµ·å§‹ URL
		c.urlQueue <- URL{Address: startURL, Depth: 0}
		
		// ç­‰å¾…å®Œæˆåå…³é—­ channels
		go func() {
			c.wg.Wait()
			close(c.results)
		}()
		
		return c.results
	}
	
	// åˆ›å»ºçˆ¬è™«
	crawler := NewCrawler(3, 2)
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()
	
	// å¼€å§‹çˆ¬å–
	results := start(crawler, ctx, "https://example.com")
	
	// æ”¶é›†ç»“æœ
	pageCount := 0
	for page := range results {
		pageCount++
		fmt.Printf("ğŸ“„ é¡µé¢ %d: %s (%d ä¸ªé“¾æ¥)\n", pageCount, page.URL, len(page.Links))
	}
	
	fmt.Printf("\næ€»å…±æŠ“å– %d ä¸ªé¡µé¢\n", pageCount)
}

func main() {
	rand.Seed(time.Now().UnixNano())
	
	fmt.Println("======================================")
	fmt.Println("    å¹¶å‘ç½‘é¡µçˆ¬è™«å®æˆ˜")
	fmt.Println("======================================")
	
	example1_simple_scraper()
	example2_rate_limited_scraper()
	example3_depth_first_crawler()
	example4_breadth_first_crawler()
	example5_scraper_with_retry()
	example6_complete_crawler()
	
	fmt.Println("\næ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
}
